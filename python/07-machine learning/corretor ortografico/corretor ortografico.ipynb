{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O arquivo \"artigos.txt\" foi carregado e possui 2605046 caracteres.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "arquivo = io.open('artigos.txt', 'r', encoding='utf-8')\n",
    "artigos = arquivo.read()\n",
    "print(f'O arquivo \"artigos.txt\" foi carregado e possui {len(artigos)} caracteres.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joaoteixeira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O arquivo \"artigos.txt\" possui 403031 palavras.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def separa_palavras(lista_tokens):\n",
    "    lista_palavras = []\n",
    "    for token in lista_tokens:\n",
    "        if token.isalpha():\n",
    "            lista_palavras.append(token)\n",
    "    return lista_palavras\n",
    "\n",
    "lista_tokens = nltk.tokenize.word_tokenize(artigos)\n",
    "lista_palavras = separa_palavras(lista_tokens)\n",
    "print(f'O arquivo \"artigos.txt\" possui {len(lista_palavras)} palavras.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O arquivo \"artigos.txt\" possui 18464 palavras únicas.\n",
      "Exemplo: ['imagem', 'temos', 'a', 'seguinte', 'classe', 'que', 'representa', 'um', 'usuário', 'no'] ...\n"
     ]
    }
   ],
   "source": [
    "def normalizacao(lista_palavras):\n",
    "    lista_normalizada = []\n",
    "    for palavra in lista_palavras:\n",
    "        lista_normalizada.append(palavra.lower())\n",
    "    return lista_normalizada\n",
    "\n",
    "lista_normalizada = normalizacao(lista_palavras)\n",
    "palavras_unicas = set(lista_normalizada)\n",
    "print(f'O arquivo \"artigos.txt\" possui {len(palavras_unicas)} palavras únicas.')\n",
    "print(f'Exemplo: {lista_normalizada[:10]} ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As 10 palavras que mais ocorrem são: [('de', 15502), ('o', 14056), ('que', 12230), ('a', 11099), ('e', 10501), ('para', 7710), ('um', 6367), ('é', 5899), ('uma', 5220), ('do', 5124)]\n"
     ]
    }
   ],
   "source": [
    "frequencia = nltk.FreqDist(lista_normalizada)\n",
    "print(f'As 10 palavras que mais ocorrem são: {frequencia.most_common(10)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O arquivo \"palavras.txt\" foi carregado e possui 186 pares de palavras para testes de eficiência do corretor.\n",
      "Exemplo: [('podemos', 'pyodemos'), ('esse', 'esje'), ('já', 'jrá'), ('nosso', 'nossov'), ('são', 'sãêo')] ...\n"
     ]
    }
   ],
   "source": [
    "def cria_dados_teste(nome_arquivo):\n",
    "    lista_palavras_teste = []\n",
    "    f = open(nome_arquivo, 'r', encoding='utf-8')\n",
    "    for linha in f:\n",
    "        correta, errada = linha.split()\n",
    "        lista_palavras_teste.append((correta, errada))\n",
    "    f.close()\n",
    "    return lista_palavras_teste\n",
    "\n",
    "lista_teste = cria_dados_teste('palavras.txt')\n",
    "print(f'O arquivo \"palavras.txt\" foi carregado e possui {len(lista_teste)} pares de palavras para testes de eficiência do corretor.')\n",
    "print(f'Exemplo: {lista_teste[:5]} ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_palavras = len(lista_normalizada)\n",
    "letras = 'abcdefghijklmnopqrstuvwxyzáâàãéêèẽíîìĩóôõòúûùũç'\n",
    "\n",
    "def insere_letras(fatias):\n",
    "    novas_palavras = []\n",
    "    for E, D in fatias:\n",
    "        for letra in letras:\n",
    "            novas_palavras.append(E + letra + D)\n",
    "    return novas_palavras\n",
    "\n",
    "def deleta_letras(fatias):\n",
    "    novas_palavras = []\n",
    "    for E, D in fatias:\n",
    "        novas_palavras.append(E + D[1:])\n",
    "    return novas_palavras\n",
    "\n",
    "def troca_letras(fatias):\n",
    "    novas_palavras = []\n",
    "    for E, D in fatias:\n",
    "        for letra in letras:\n",
    "            novas_palavras.append(E + letra + D[1:])\n",
    "    return novas_palavras\n",
    "\n",
    "def inverte_letras(fatias):\n",
    "    novas_palavras = []\n",
    "    for E, D in fatias:\n",
    "        if len(D) > 1:\n",
    "            novas_palavras.append(E + D[1] + D[0] + D[2:])\n",
    "    return novas_palavras\n",
    "\n",
    "def gerador_palavras_1(palavra):\n",
    "    fatias = []\n",
    "    for i in range(len(palavra)+1):\n",
    "        fatias.append((palavra[:i],palavra[i:]))\n",
    "    palavras_geradas = insere_letras(fatias)\n",
    "    palavras_geradas += deleta_letras(fatias)\n",
    "    palavras_geradas += troca_letras(fatias)\n",
    "    palavras_geradas += inverte_letras(fatias)\n",
    "    return palavras_geradas\n",
    "\n",
    "def gerador_palavras_2(palavras_geradas):\n",
    "    novas_palavras = []\n",
    "    for palavra in palavras_geradas:\n",
    "        novas_palavras += gerador_palavras_1(palavra)\n",
    "    return novas_palavras\n",
    "\n",
    "def probabilidade(palavra_gerada):\n",
    "    return frequencia[palavra_gerada]/total_palavras\n",
    "\n",
    "def corretor_dist1(palavra):\n",
    "    palavras_geradas = gerador_palavras_1(palavra)\n",
    "    palavra_correta = max(palavras_geradas, key=probabilidade)\n",
    "    return palavra_correta\n",
    "\n",
    "def corretor_dist2(palavra):\n",
    "    palavras_geradas_1 = gerador_palavras_1(palavra)\n",
    "    palavras_geradas_2 = gerador_palavras_2(palavras_geradas_1)\n",
    "    todas_palavras = set(palavras_geradas_1 + palavras_geradas_2)\n",
    "    candidatos = [palavra]\n",
    "    for p in todas_palavras:\n",
    "        if p in palavras_unicas:\n",
    "            candidatos.append(p)\n",
    "    palavra_correta = max(candidatos, key=probabilidade)\n",
    "    return palavra_correta\n",
    "\n",
    "def avaliador(testes):\n",
    "    num_palavras = len(testes)\n",
    "    acertou = 0\n",
    "    desconhecidas = 0\n",
    "    for correta, errada in testes:\n",
    "        palavra_corrigida = corretor_dist2(errada)\n",
    "        if correta not in palavras_unicas:\n",
    "            desconhecidas +=1\n",
    "            palavras_unicas.add(correta)\n",
    "        if palavra_corrigida == correta:\n",
    "            acertou += 1\n",
    "        else:\n",
    "            palavra_corrigida = corretor_dist1(errada)\n",
    "            if palavra_corrigida  == correta:\n",
    "                acertou += 1\n",
    "    taxa_palavras_desconhecidas = round((desconhecidas/num_palavras)*100,2) \n",
    "    taxa_acerto = round((acertou/num_palavras)*100,2)\n",
    "    print(f'A taxa de acerto é: {taxa_acerto}%')\n",
    "    print(f'A taxa de palavras desconhecidas é: {taxa_palavras_desconhecidas}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A taxa de acerto é: 85.48%\n",
      "A taxa de palavras desconhecidas é: 5.38%\n"
     ]
    }
   ],
   "source": [
    "avaliador(lista_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lógica\n",
      "lógica\n"
     ]
    }
   ],
   "source": [
    "palavra_teste = 'lóigca'    # os dois coretores acertam\n",
    "print(corretor_dist1(palavra_teste))\n",
    "print(corretor_dist2(palavra_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alóigcca\n",
      "lógica\n"
     ]
    }
   ],
   "source": [
    "palavra_teste = 'lóigcca'   # apenas o corretor 2 acerta\n",
    "print(corretor_dist1(palavra_teste))\n",
    "print(corretor_dist2(palavra_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esse\n",
      "se\n"
     ]
    }
   ],
   "source": [
    "palavra_teste = 'esze'      # apenas o corretor 1 acerta\n",
    "print(corretor_dist1(palavra_teste))\n",
    "print(corretor_dist2(palavra_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deste\n",
      "dessa\n"
     ]
    }
   ],
   "source": [
    "palavra_teste = 'deset'     # apenas o corretor 1 acerta\n",
    "print(corretor_dist1(palavra_teste))\n",
    "print(corretor_dist2(palavra_teste))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "390b26408c1a9fab7aa60534330aa585819becb932e463410786038f7884dffb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
